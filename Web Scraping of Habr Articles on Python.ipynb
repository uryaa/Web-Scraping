{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d581cc61",
   "metadata": {},
   "source": [
    "## Project Description:\n",
    "\n",
    "This project involves developing a web scraper to collect articles from the Habr website based on specific search queries such as \"python\" and \"data analysis.\" The scraper navigates through the search results pages, extracts the titles, links, publication dates, full text of the articles, and the number of likes or votes. The collected data is organized into a structured pandas DataFrame for further analysis or use. This automation provides a streamlined way to gather information from Habr for research or content aggregation purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f836b8",
   "metadata": {
    "id": "50f836b8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4418405",
   "metadata": {},
   "source": [
    "## Определение функции get_habr_posts(query, pages)\n",
    "\n",
    "Описание: Эта функция выполняет веб-скрапинг статей с сайта Habr по заданным ключевым словам (в query) и возвращает данные в виде таблицы. Для каждого ключевого слова извлекается информация с указанного количества страниц (в pages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "085358c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_habr_posts(query, pages):\n",
    "    # Initialize an empty DataFrame to store the results of the scraped data\n",
    "    habr_blog = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each search query term provided in the 'query' list\n",
    "    for q in query:\n",
    "        x = 1\n",
    "        # Loop through the specified number of pages\n",
    "        while x <= pages:\n",
    "            \n",
    "            # Form the URL for the Habr search page with pagination\n",
    "            URL = f'https://habr.com/ru/search/page{x}/'\n",
    "            params = {'q': q}  # Define the search query parameters\n",
    "            \n",
    "            # Send a GET request to the Habr search page with the query\n",
    "            req = requests.get(URL, params=params)\n",
    "            \n",
    "            # To avoid being blocked, introduce a slight delay between requests\n",
    "            time.sleep(0.3)\n",
    "            \n",
    "            # Parse the HTML content of the page with BeautifulSoup\n",
    "            soup = BeautifulSoup(req.text)\n",
    "            \n",
    "            # Find all article elements on the page\n",
    "            articles = soup.find_all('article', class_='tm-articles-list__item')\n",
    "            \n",
    "            # Iterate through each article on the current page\n",
    "            for article in articles:\n",
    "                # Extract the article's title based on its HTML structure\n",
    "                if article.find('a', class_='tm-article-snippet__title-link'):\n",
    "                    title = article.find('a', class_='tm-article-snippet__title-link').find('span').text\n",
    "                else:\n",
    "                    title = article.find('h2', class_='tm-megapost-snippet__title').text\n",
    "                \n",
    "                # Extract the link to the article\n",
    "                if article.find('a', class_='tm-article-snippet__title-link'):\n",
    "                    link = article.find('a', class_='tm-article-snippet__title-link').get('href')\n",
    "                    link = 'https://habr.com' + link\n",
    "                else:\n",
    "                    link = article.find('header', class_='tm-megapost-snippet__header').find('a').get('href')\n",
    "                    link = 'https://habr.com' + link\n",
    "                \n",
    "                # Send a GET request to the full article page to extract the article's text\n",
    "                req_2 = requests.get(link)\n",
    "                soup_2 = BeautifulSoup(req_2.text)\n",
    "                \n",
    "                # Extract the main body text of the article based on different formats\n",
    "                if soup_2.find('div', class_=\"article-formatted-body article-formatted-body article-formatted-body_version-1\"):\n",
    "                    text = soup_2.find('div', class_=\"article-formatted-body article-formatted-body article-formatted-body_version-1\").text.strip()\n",
    "                elif soup_2.find('div', class_='t-records'):\n",
    "                    text = soup_2.find('div', class_='t-records').text.strip()\n",
    "                else:\n",
    "                    text = soup_2.find('p').text.strip()\n",
    "                \n",
    "                # Extract the publication date of the article\n",
    "                date = article.find('time').get('title')\n",
    "                \n",
    "                # Extract the number of votes/likes the article received\n",
    "                votes = article.find('svg', class_=\"tm-svg-img tm-votes-meter__icon tm-votes-meter__icon tm-votes-meter__icon_appearance-article\").text\n",
    "                votes = votes[17:]  # Clean the votes data\n",
    "                \n",
    "                # Create a dictionary with all the scraped information for the article\n",
    "                row = {'date': date, 'title': title, 'link': link, 'full text': text, 'likes': votes}\n",
    "                \n",
    "                # Append the dictionary as a row to the DataFrame\n",
    "                habr_blog = pd.concat([habr_blog, pd.DataFrame([row])])\n",
    "\n",
    "            # Move to the next page of search results\n",
    "            x = x + 1\n",
    "    \n",
    "    # Return the DataFrame with all the collected data, resetting the index\n",
    "    return habr_blog.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931605d",
   "metadata": {
    "id": "8931605d"
   },
   "outputs": [],
   "source": [
    "# Call the function with specific queries and scrape one page of results for each query\n",
    "df = get_habr_posts(['python','анализ данных'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b8919",
   "metadata": {
    "id": "3f0b8919",
    "outputId": "b0cd8e68-1c21-4dc2-e1b5-a3ef0c51f99b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>full text</th>\n",
       "      <th>likes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-07-25, 13:29</td>\n",
       "      <td>Пять лет Школе анализа данных</td>\n",
       "      <td>https://habr.com/ru/company/yandex/blog/148443/</td>\n",
       "      <td>Ровно пять лет назад Яндекс объявил об открыти...</td>\n",
       "      <td>↑25 и ↓4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-06-18, 15:19</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/226641/</td>\n",
       "      <td>Данный выпуск дайджеста наиболее интересных ма...</td>\n",
       "      <td>↑22 и ↓3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-06-30, 23:06</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/228187/</td>\n",
       "      <td>Данный выпуск обзора наиболее интересных матер...</td>\n",
       "      <td>↑26 и ↓2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-07-28, 13:10</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/231323/</td>\n",
       "      <td>В очередном выпуске обзора наиболее интересных...</td>\n",
       "      <td>↑21 и ↓1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-11, 10:47</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/232879/</td>\n",
       "      <td>Представляю вашему вниманию очередной выпуск о...</td>\n",
       "      <td>↑23 и ↓0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2014-09-14, 19:54</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/236757/</td>\n",
       "      <td>Представляю вашему вниманию очередной выпуск о...</td>\n",
       "      <td>↑26 и ↓1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014-10-13, 14:00</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/240139/</td>\n",
       "      <td>Представляю вашему вниманию очередной выпуск о...</td>\n",
       "      <td>↑20 и ↓1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2014-11-23, 16:49</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/243967/</td>\n",
       "      <td>Представляю вашему вниманию очередной выпуск о...</td>\n",
       "      <td>↑20 и ↓2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2015-01-18, 18:02</td>\n",
       "      <td>Обзор наиболее интересных материалов по анализ...</td>\n",
       "      <td>https://habr.com/ru/post/248165/</td>\n",
       "      <td>Представляю вашему вниманию очередной выпуск о...</td>\n",
       "      <td>↑17 и ↓0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2017-06-27, 13:51</td>\n",
       "      <td>Использование Python и Excel для обработки и а...</td>\n",
       "      <td>https://habr.com/ru/company/otus/blog/331746/</td>\n",
       "      <td>Если Вы только начинаете свой путь знакомства ...</td>\n",
       "      <td>↑12 и ↓4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                date                                              title  \\\n",
       "0  2012-07-25, 13:29                      Пять лет Школе анализа данных   \n",
       "1  2014-06-18, 15:19  Обзор наиболее интересных материалов по анализ...   \n",
       "2  2014-06-30, 23:06  Обзор наиболее интересных материалов по анализ...   \n",
       "3  2014-07-28, 13:10  Обзор наиболее интересных материалов по анализ...   \n",
       "4  2014-08-11, 10:47  Обзор наиболее интересных материалов по анализ...   \n",
       "5  2014-09-14, 19:54  Обзор наиболее интересных материалов по анализ...   \n",
       "6  2014-10-13, 14:00  Обзор наиболее интересных материалов по анализ...   \n",
       "7  2014-11-23, 16:49  Обзор наиболее интересных материалов по анализ...   \n",
       "8  2015-01-18, 18:02  Обзор наиболее интересных материалов по анализ...   \n",
       "9  2017-06-27, 13:51  Использование Python и Excel для обработки и а...   \n",
       "\n",
       "                                              link  \\\n",
       "0  https://habr.com/ru/company/yandex/blog/148443/   \n",
       "1                 https://habr.com/ru/post/226641/   \n",
       "2                 https://habr.com/ru/post/228187/   \n",
       "3                 https://habr.com/ru/post/231323/   \n",
       "4                 https://habr.com/ru/post/232879/   \n",
       "5                 https://habr.com/ru/post/236757/   \n",
       "6                 https://habr.com/ru/post/240139/   \n",
       "7                 https://habr.com/ru/post/243967/   \n",
       "8                 https://habr.com/ru/post/248165/   \n",
       "9    https://habr.com/ru/company/otus/blog/331746/   \n",
       "\n",
       "                                           full text       likes  \n",
       "0  Ровно пять лет назад Яндекс объявил об открыти...   ↑25 и ↓4   \n",
       "1  Данный выпуск дайджеста наиболее интересных ма...   ↑22 и ↓3   \n",
       "2  Данный выпуск обзора наиболее интересных матер...   ↑26 и ↓2   \n",
       "3  В очередном выпуске обзора наиболее интересных...   ↑21 и ↓1   \n",
       "4  Представляю вашему вниманию очередной выпуск о...   ↑23 и ↓0   \n",
       "5  Представляю вашему вниманию очередной выпуск о...   ↑26 и ↓1   \n",
       "6  Представляю вашему вниманию очередной выпуск о...   ↑20 и ↓1   \n",
       "7  Представляю вашему вниманию очередной выпуск о...   ↑20 и ↓2   \n",
       "8  Представляю вашему вниманию очередной выпуск о...   ↑17 и ↓0   \n",
       "9  Если Вы только начинаете свой путь знакомства ...   ↑12 и ↓4   "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate entries based on the 'link' column\n",
    "df.drop_duplicates(inplace=True, subset=['link'])\n",
    "\n",
    "# Sort the DataFrame by the publication date of the articles\n",
    "df.sort_values(by=['date'], inplace=True)\n",
    "\n",
    "# Reset the index of the sorted DataFrame\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the first 10 entries of the DataFrame\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0359999e",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This web scraping project successfully automates the collection of article data from the Habr website. It effectively captures essential details such as the article's title, content, publication date, and user votes. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
